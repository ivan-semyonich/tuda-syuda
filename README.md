# Taming the Sine Line
Задачка с телегой в долине

## Core ideas

Решение — нормальный такой дип ку-лёрнинг, в обучение добавлен некоторый читинг (ну или хакинг или шейпинг или трюки, если угодно).

![q-map](https://raw.githubusercontent.com/ivan-semyonich/tuda-syuda/master/pics/actions_space_1586042498.png?token=APBETR5YV66IYFYLIT4QQF26REPRE "q-map")

1. Чтобы агент побыстрее научился отползать с мёртвой точки и учился не целую вечность, 
**хвалим его за прирост в скорости**.
2. [**ОТКЛЮЧЕНО**] В начале обучения в историю мы сохраняем в том числе симметричные случаи 
(хоть задача и несимметрична, "раскачка" между холмами полезна).
3. В одном месте к признакам состояния добавляем их же, возведённые в квадрат (потому что если бы я писал эвристику, 
там бы были квадраты расстояния; так пусть машина сама подберёт что-то подобное). Ненужные члены полинома элиминируем 
гейтом, то есть линейным слоем + сигмоидой.
4. Близость до флажка решил не добавлять в обновлённый реворд, уж совсем читерство.

Пробовал что-то вроде Brain Damage, тупое усреднение, добавление шума в веса, добавление шума в данные (вообще оно-то 
как раз должно было помочь, потому что у позиции и скорости есть свойство локальности) — и ещё много что. Кое-что удалил, 
кое-что осталось в виде закомментированного кода (но я старался, чтобы было аккуратно).

Сохраняю раз в несколько шагов "карту" выученного поведения агента, а также историю ревордов. Довольно прикольно — 
и много рассказывает о задаче и пространствах решений.

## Howto

- точка входа — `main.py`
- архитектура агента и её обёртка — в `model.py`
- обновлённая награда — в `training.py`
- память (просто циклический буфер) — в  `memory.py`
- рисование картинок — в `utils.py`

![training](https://raw.githubusercontent.com/ivan-semyonich/tuda-syuda/master/pics/results_training_1586042492.png?token=APBETRYNP3UCBFVVJJPAO226REPKK "Training process")

Бонусом пара неравенств, которые решают задачу, взяты с интернета 
и в виде готового кода  лежат в файле `deterministic_policy_for_comparison.py`.

![kleinbottle](https://raw.githubusercontent.com/ivan-semyonich/tuda-syuda/master/pics/actions_space_DETERMINISTIC_POLICY.png?token=APBETRYB32GILKTPAOPFCU26REOZS "Here you are")

Ещё один бонус — интересные и не очень картинки, которые получились в процессе.

![rewardpain](https://raw.githubusercontent.com/ivan-semyonich/tuda-syuda/master/pics/elephants_start_mating-when_you_pull_reward_shaping_levers_too_hard.png?token=APBETR6WHXQBOI7UOHCRC2C6REO64 "Кое-кто перестарался с ревордом")

*Ситуация, когда вознаграждение агенту слишком сильно отличается от исходного. Сделать хотел утюг — слон получился вдруг.*